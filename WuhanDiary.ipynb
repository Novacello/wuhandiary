{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import defaultdict\n",
    "\n",
    "# 财新网保存有《方方日记》60篇，网址如下\n",
    "base_url = r\"http://m.app.caixin.com/m_topic_detail/1489.html\"\n",
    "\n",
    "#获取每篇日记的网址，并保存在一个字典里（方便后面生成dataframe）\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.95 Safari/537.36'}\n",
    "base_page_content = requests.get(base_url, headers=headers).text\n",
    "soup = BeautifulSoup(base_page_content, \"lxml\")\n",
    "diary_data = defaultdict(list)\n",
    "\n",
    "# 观察base_page可知，日记相关信息都在<li></li>标签内。解析所有<li></li>标签。\n",
    "for index, li in enumerate(soup.find_all('li')):\n",
    "    try:\n",
    "        # 保存日记发表日期，标题和网址为一个元组\n",
    "        diary_data[index] = [li.em.text, li.a['title'], li.a['href']]\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "# 定义一个函数，获取日记正文\n",
    "def get_diary(url):\n",
    "    time.sleep(0.5) # 间隔0.5秒，以免超过限制\n",
    "    headers = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.95 Safari/537.36'}\n",
    "    try:\n",
    "        diary_page = requests.get(url).text\n",
    "        soup = BeautifulSoup(diary_page, 'lxml')\n",
    "        content = soup.find('div', {'class': 'blog_content'}).text\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "    return content\n",
    "\n",
    "for key in diary_data:\n",
    "    url = diary_data[key][2]\n",
    "    content = get_diary(url)[:-19] # 每个页面结尾有18个重复的和日记无关的字符，去掉。\n",
    "    # 同时保存一个文本文件,编码为gbk，因为百度情感倾向分析只认gbk。\n",
    "    with open(diary_data[key][1]+'\\.txt', 'w', encoding='gbk') as f:\n",
    "        f.write(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来是清理数据，改文件名。因为发表日期和日记日期不一定相同，而且方方有时候用公历，有时候用就离，需要一些手动的操作。最后文件名统一格式为\"nn-yyyymmdd.txt\"。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 连接百度情感倾向分析API\n",
    "from aip import AipNlp\n",
    "\n",
    "APP_ID = 'yourid'\n",
    "API_KEY = 'your api key'\n",
    "SECRET_KEY = 'your secret key'\n",
    "\n",
    "client = AipNlp(APP_ID, API_KEY, SECRET_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义两个函数，用于获取情感倾向值\n",
    "\n",
    "import collections\n",
    "from statistics import mean\n",
    "\n",
    "def get_sentiment(senti_list):\n",
    "    # 本函数获取一个文本的情感倾向\n",
    "    # 因为情感倾向是定名数据，不能取平均值，只能取频率最高的\n",
    "    counter = collections.Counter(senti_list)\n",
    "    if counter[0] == counter[1] == counter[2]:\n",
    "        sentiment = 1\n",
    "    else:\n",
    "        sentiment = counter.most_common()[0][0]\n",
    "    return sentiment\n",
    "        \n",
    "def analyzeSentiment(text):\n",
    "    with open(text, 'r', encoding='gbk') as f:\n",
    "            content = f.read()\n",
    "    # 百度舆情分析一次不能超过2047个字节，因此切割为500个字符的块，每篇日记取算术平均值\n",
    "    if len(content) <= 500:\n",
    "        result = client.sentimentClassify(content)\n",
    "        print(result['items'])\n",
    "        positive_prob = result['items'][0]['positive_prob']\n",
    "        negative_prob = result['items'][0]['negative_prob']\n",
    "        sentiment = result['items'][0]['sentiment']\n",
    "        confidence = result['items'][0]['confidence']\n",
    "    else:\n",
    "        k = len(content) // 500\n",
    "        positive_prob_list = []\n",
    "        negative_prob_list = []\n",
    "        sentiment_list = []\n",
    "        confidence_list = []\n",
    "        # 先处理文本最后小于500词的片段\n",
    "        text = content[k*500:]\n",
    "        result = client.sentimentClassify(text)\n",
    "        print(result['items'])\n",
    "        positive_prob_list.append(result['items'][0]['positive_prob'])\n",
    "        negative_prob_list.append(result['items'][0]['negative_prob'])\n",
    "        sentiment_list.append(result['items'][0]['sentiment'])\n",
    "        confidence_list.append(result['items'][0]['confidence'])\n",
    "        # 再处理前面k个500词的片段\n",
    "        for i in range(k):\n",
    "            time.sleep(0.6)\n",
    "            text = content[i*500:i*500+500]\n",
    "            result = client.sentimentClassify(text)\n",
    "            print(result['items'])\n",
    "            positive_prob_list.append(result['items'][0]['positive_prob'])\n",
    "            negative_prob_list.append(result['items'][0]['negative_prob'])\n",
    "            sentiment_list.append(result['items'][0]['sentiment'])\n",
    "            confidence_list.append(result['items'][0]['confidence'])\n",
    "        positive_prob = mean(positive_prob_list)\n",
    "        negative_prob = mean(negative_prob_list)\n",
    "        sentiment = get_sentiment(sentiment_list)\n",
    "        confidence = mean(confidence_list)\n",
    "    return [positive_prob, negative_prob, sentiment, confidence]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取文本，获取情感倾向值\n",
    "\n",
    "import time\n",
    "import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "os.chdir(r'your directory')\n",
    "diaries = [diary for diary in glob.glob('*.txt') if '-' in diary]\n",
    "\n",
    "df = pd.DataFrame(\n",
    "    columns=['number', 'date', 'positive_prob', 'negative_prob', 'sentiment', 'confidence'],\n",
    ")\n",
    "\n",
    "for diary in diaries:\n",
    "    time.sleep(0.6) # 百度舆情分析每秒有两次限制，因此设定间隔0.6秒\n",
    "    print('Processing '+diary)\n",
    "    number = diary.split('-')[0] # 文件名格式为nn-yyyymmdd.txt\n",
    "    date = diary.split('.')[0][3:]\n",
    "    senti_values = analyzeSentiment(diary)\n",
    "    df_new = pd.DataFrame(\n",
    "        np.array([[number, date, senti_values[0], senti_values[1], senti_values[2], senti_values[3]]]),\n",
    "        columns=['number', 'date', 'positive_prob', 'negative_prob', 'sentiment', 'confidence'],\n",
    "    )\n",
    "    df = df.append(df_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.set_index('number')\n",
    "\n",
    "# 因为上面没有设置dataframe里的数据类型，这里分别设置一下\n",
    "# 除date设置为日期格式以外，其它列都设置为数值\n",
    "\n",
    "for c in df.columns:\n",
    "    if c == 'date':\n",
    "        df[c] = pd.to_datetime(df[c])\n",
    "    else:\n",
    "        df[c] = pd.to_numeric(df[c])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 现在可以看一下积极、消极概率的折线图了\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [20, 8]\n",
    "\n",
    "number = df.index\n",
    "plt.xticks = number\n",
    "pp = df['positive_prob']\n",
    "np = df['negative_prob']\n",
    "sent = df['sentiment']\n",
    "plt.plot(number, pp, color='red')\n",
    "plt.plot(number, np, color='green')\n",
    "#plt.plot(number, sent, color='black')\n",
    "    \n",
    "plt.title('Sentiment Analysis', fontsize=20)\n",
    "plt.xlabel('diary', fontsize=15)\n",
    "plt.ylabel('probability', fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment = ('negtive', 'neutral', 'positive')\n",
    "y_pos = np.arange(len(sentiment))\n",
    "freq = [41,1,18]\n",
    "\n",
    "plt.bar(sentiment, freq, align='center', alpha=0.5)\n",
    "#plt.xticks(y_pos, sentiment)\n",
    "plt.ylabel('freq')\n",
    "plt.title('Sentiment Analysis')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "filenames = ['01-20200125.txt', '23-20200216.txt', '32-20200225.txt', \n",
    "             '15-20200208.txt', '27-20200220.txt', '58-20200322.txt']\n",
    "fontpath = r'msyh.ttc'\n",
    "\n",
    "def make_cloud(file):\n",
    "    with open('cn_stopwords.txt', 'r', encoding='utf8') as f:\n",
    "        stopwords = f.readlines()\n",
    "    stopwords = [word.strip() for word in stopwords]\n",
    "    with open(file, 'r') as f:\n",
    "        text = f.read()\n",
    "    text = \" \".join([word for word in jieba.cut(text) if word not in stopwords])\n",
    "    wordcloud = WordCloud(font_path=fontpath).generate(text)\n",
    "    return wordcloud\n",
    "    \n",
    "\n",
    "for file in filenames:\n",
    "    wordcloud = make_cloud(file)\n",
    "    print('wordcloud for {}'.format(file))\n",
    "    %pylab inline\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
